{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 1\n",
      "[[10435.55]\n",
      " [10442.2 ]\n",
      " [10443.2 ]\n",
      " [10504.8 ]\n",
      " [10558.85]\n",
      " [10623.6 ]\n",
      " [10637.  ]\n",
      " [10632.2 ]\n",
      " [10651.2 ]\n",
      " [10681.25]\n",
      " [10741.55]\n",
      " [10700.45]\n",
      " [10788.55]\n",
      " [10817.  ]\n",
      " [10894.7 ]\n",
      " [10966.2 ]\n",
      " [11083.7 ]\n",
      " [11086.  ]\n",
      " [11069.65]\n",
      " [11130.4 ]\n",
      " [11049.65]\n",
      " [11027.7 ]\n",
      " [11016.9 ]\n",
      " [10760.6 ]\n",
      " [10666.55]\n",
      " [10498.25]\n",
      " [10476.7 ]\n",
      " [10576.85]\n",
      " [10454.95]\n",
      " [10539.75]\n",
      " [10500.9 ]\n",
      " [10545.5 ]\n",
      " [10452.3 ]\n",
      " [10378.4 ]\n",
      " [10360.4 ]\n",
      " [10397.45]\n",
      " [10382.7 ]\n",
      " [10491.05]\n",
      " [10582.6 ]\n",
      " [10554.3 ]\n",
      " [10492.85]\n",
      " [10458.35]\n",
      " [10358.85]\n",
      " [10249.25]\n",
      " [10154.2 ]\n",
      " [10242.65]\n",
      " [10226.85]\n",
      " [10421.4 ]\n",
      " [10426.85]\n",
      " [10410.9 ]\n",
      " [10360.15]\n",
      " [10195.15]\n",
      " [10094.25]\n",
      " [10124.35]\n",
      " [10155.25]\n",
      " [10114.75]\n",
      " [ 9998.05]\n",
      " [10130.65]\n",
      " [10184.15]\n",
      " [10113.7 ]\n",
      " [10211.8 ]\n",
      " [10245.  ]\n",
      " [10128.4 ]\n",
      " [10325.15]\n",
      " [10331.6 ]\n",
      " [10379.35]\n",
      " [10402.25]\n",
      " [10417.15]\n",
      " [10458.65]\n",
      " [10480.6 ]\n",
      " [10528.35]\n",
      " [10548.7 ]\n",
      " [10526.2 ]\n",
      " [10565.3 ]\n",
      " [10564.05]\n",
      " [10584.7 ]\n",
      " [10614.35]\n",
      " [10570.55]\n",
      " [10617.8 ]\n",
      " [10692.3 ]\n",
      " [10739.35]\n",
      " [10718.05]\n",
      " [10679.65]\n",
      " [10618.25]\n",
      " [10715.5 ]\n",
      " [10717.8 ]\n",
      " [10741.7 ]\n",
      " [10716.55]\n",
      " [10806.5 ]\n",
      " [10806.6 ]\n",
      " [10801.85]\n",
      " [10741.1 ]\n",
      " [10682.7 ]\n",
      " [10596.4 ]\n",
      " [10516.7 ]\n",
      " [10536.7 ]\n",
      " [10430.35]\n",
      " [10513.85]\n",
      " [10605.15]\n",
      " [10688.65]\n",
      " [10633.3 ]\n",
      " [10614.35]\n",
      " [10736.15]\n",
      " [10696.2 ]\n",
      " [10628.5 ]\n",
      " [10593.15]\n",
      " [10684.65]\n",
      " [10768.35]\n",
      " [10767.65]\n",
      " [10786.95]\n",
      " [10842.85]\n",
      " [10856.7 ]\n",
      " [10808.05]\n",
      " [10817.7 ]\n",
      " [10799.85]\n",
      " [10710.45]\n",
      " [10772.05]\n",
      " [10741.1 ]\n",
      " [10821.85]\n",
      " [10762.45]\n",
      " [10769.15]\n",
      " [10671.4 ]\n",
      " [10589.1 ]\n",
      " [10714.3 ]\n",
      " [10657.3 ]\n",
      " [10699.9 ]\n",
      " [10769.9 ]\n",
      " [10749.75]\n",
      " [10772.65]\n",
      " [10852.9 ]\n",
      " [10947.25]\n",
      " [10948.3 ]\n",
      " [11023.2 ]\n",
      " [11018.9 ]\n",
      " [10936.85]\n",
      " [11008.05]\n",
      " [10980.45]\n",
      " [10957.1 ]\n",
      " [11010.2 ]\n",
      " [11084.75]\n",
      " [11134.3 ]\n",
      " [11132.  ]\n",
      " [11167.3 ]\n",
      " [11278.35]\n",
      " [11319.55]\n",
      " [11356.5 ]\n",
      " [11346.2 ]\n",
      " [11244.7 ]\n",
      " [11360.8 ]\n",
      " [11387.1 ]\n",
      " [11389.45]\n",
      " [11450.  ]\n",
      " [11470.7 ]\n",
      " [11429.5 ]\n",
      " [11355.75]\n",
      " [11435.1 ]\n",
      " [11385.05]\n",
      " [11470.75]\n",
      " [11551.75]\n",
      " [11570.9 ]\n",
      " [11582.75]\n",
      " [11557.1 ]\n",
      " [11691.95]\n",
      " [11738.5 ]\n",
      " [11691.9 ]\n",
      " [11676.8 ]\n",
      " [11680.5 ]\n",
      " [11582.35]\n",
      " [11520.3 ]\n",
      " [11476.95]\n",
      " [11536.9 ]\n",
      " [11589.1 ]\n",
      " [11438.1 ]\n",
      " [11287.5 ]\n",
      " [11369.9 ]\n",
      " [11515.2 ]\n",
      " [11377.75]\n",
      " [11278.9 ]\n",
      " [11234.35]\n",
      " [11143.1 ]\n",
      " [10967.4 ]\n",
      " [11067.45]\n",
      " [11053.8 ]\n",
      " [10977.55]\n",
      " [10930.45]\n",
      " [11008.3 ]\n",
      " [10858.25]\n",
      " [10599.25]\n",
      " [10316.45]\n",
      " [10348.05]\n",
      " [10301.05]\n",
      " [10460.1 ]\n",
      " [10234.65]\n",
      " [10472.5 ]\n",
      " [10512.5 ]\n",
      " [10584.75]\n",
      " [10453.05]\n",
      " [10303.55]\n",
      " [10245.25]\n",
      " [10146.8 ]\n",
      " [10224.75]\n",
      " [10124.9 ]\n",
      " [10030.  ]\n",
      " [10250.85]\n",
      " [10198.4 ]\n",
      " [10386.6 ]\n",
      " [10380.45]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('NIFTY.csv')\n",
    "data = data.drop(['Date'], 1)\n",
    "n = data.shape[0]\n",
    "print(data.shape[0], data.shape[1])\n",
    "data = data.values\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.00190839694656475, 0.11946564885495903, 0.22261450381679282, 0.34618320610686837, 0.37175572519083744, 0.36259541984732735, 0.39885496183206115, 0.45620229007633384, 0.5712786259541929, 0.4928435114503813, 0.6609732824427432, 0.7152671755725173, 0.863549618320608] 1\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_slices = []\n",
    "y_slices = []\n",
    "for i in range(n-15):\n",
    "    current_slice = data[i:i+15]\n",
    "    current_slice = scaler.fit_transform(current_slice)\n",
    "    data_slices.append([ piece[0] for piece in current_slice[:14] ])\n",
    "    movement = current_slice[-1][0] - current_slice[-2][0]\n",
    "    y_slices.append(0 if np.sign(movement) ==  -1 else 1)\n",
    "\n",
    "print(data_slices[1], y_slices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_slices[:int(n*0.8)]\n",
    "y_train = y_slices[:int(n*0.8)]\n",
    "X_test = data_slices[int(n*0.8):]\n",
    "y_test = y_slices[int(n*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=14, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.7005 - acc: 0.4667\n",
      "Epoch 2/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6974 - acc: 0.4667\n",
      "Epoch 3/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6962 - acc: 0.4970\n",
      "Epoch 4/100\n",
      "165/165 [==============================] - 0s 121us/step - loss: 0.6949 - acc: 0.5091\n",
      "Epoch 5/100\n",
      "165/165 [==============================] - 0s 109us/step - loss: 0.6938 - acc: 0.5152\n",
      "Epoch 6/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6927 - acc: 0.5212\n",
      "Epoch 7/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6921 - acc: 0.5697\n",
      "Epoch 8/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6910 - acc: 0.5515\n",
      "Epoch 9/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6905 - acc: 0.5636\n",
      "Epoch 10/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6904 - acc: 0.5394\n",
      "Epoch 11/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6895 - acc: 0.5515\n",
      "Epoch 12/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6890 - acc: 0.5515\n",
      "Epoch 13/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6888 - acc: 0.5333\n",
      "Epoch 14/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6883 - acc: 0.5515\n",
      "Epoch 15/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6881 - acc: 0.5394\n",
      "Epoch 16/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6875 - acc: 0.5333\n",
      "Epoch 17/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6881 - acc: 0.5212\n",
      "Epoch 18/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6870 - acc: 0.5515\n",
      "Epoch 19/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6869 - acc: 0.5576\n",
      "Epoch 20/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6860 - acc: 0.5333\n",
      "Epoch 21/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6858 - acc: 0.5394\n",
      "Epoch 22/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6859 - acc: 0.5515\n",
      "Epoch 23/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6851 - acc: 0.5333\n",
      "Epoch 24/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6847 - acc: 0.5273\n",
      "Epoch 25/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6844 - acc: 0.5394\n",
      "Epoch 26/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6843 - acc: 0.5152\n",
      "Epoch 27/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6835 - acc: 0.5394\n",
      "Epoch 28/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6831 - acc: 0.5636\n",
      "Epoch 29/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6830 - acc: 0.5576\n",
      "Epoch 30/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6828 - acc: 0.5515\n",
      "Epoch 31/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6822 - acc: 0.5394\n",
      "Epoch 32/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6824 - acc: 0.5273\n",
      "Epoch 33/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6816 - acc: 0.5273\n",
      "Epoch 34/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6811 - acc: 0.5333\n",
      "Epoch 35/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6823 - acc: 0.5333\n",
      "Epoch 36/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6803 - acc: 0.5273\n",
      "Epoch 37/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6805 - acc: 0.5333\n",
      "Epoch 38/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6803 - acc: 0.5515\n",
      "Epoch 39/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6792 - acc: 0.5697\n",
      "Epoch 40/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6791 - acc: 0.5576\n",
      "Epoch 41/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6787 - acc: 0.5515\n",
      "Epoch 42/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6784 - acc: 0.5576\n",
      "Epoch 43/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6777 - acc: 0.5515\n",
      "Epoch 44/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6766 - acc: 0.5697\n",
      "Epoch 45/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6774 - acc: 0.5515\n",
      "Epoch 46/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6763 - acc: 0.5879\n",
      "Epoch 47/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6755 - acc: 0.5818\n",
      "Epoch 48/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6747 - acc: 0.5879\n",
      "Epoch 49/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6745 - acc: 0.5879\n",
      "Epoch 50/100\n",
      "165/165 [==============================] - 0s 109us/step - loss: 0.6744 - acc: 0.5939\n",
      "Epoch 51/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6732 - acc: 0.6000\n",
      "Epoch 52/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6725 - acc: 0.5818\n",
      "Epoch 53/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6719 - acc: 0.6000\n",
      "Epoch 54/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6716 - acc: 0.6121\n",
      "Epoch 55/100\n",
      "165/165 [==============================] - 0s 103us/step - loss: 0.6712 - acc: 0.6303\n",
      "Epoch 56/100\n",
      "165/165 [==============================] - 0s 115us/step - loss: 0.6707 - acc: 0.6242\n",
      "Epoch 57/100\n",
      "165/165 [==============================] - 0s 109us/step - loss: 0.6690 - acc: 0.6303\n",
      "Epoch 58/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6689 - acc: 0.6303\n",
      "Epoch 59/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6687 - acc: 0.6121\n",
      "Epoch 60/100\n",
      "165/165 [==============================] - 0s 103us/step - loss: 0.6674 - acc: 0.6121\n",
      "Epoch 61/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6669 - acc: 0.6121\n",
      "Epoch 62/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6665 - acc: 0.6242\n",
      "Epoch 63/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6654 - acc: 0.6303\n",
      "Epoch 64/100\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.6667 - acc: 0.5939\n",
      "Epoch 65/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6646 - acc: 0.6182\n",
      "Epoch 66/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6650 - acc: 0.6182\n",
      "Epoch 67/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6631 - acc: 0.6242\n",
      "Epoch 68/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6631 - acc: 0.6242\n",
      "Epoch 69/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6637 - acc: 0.6303\n",
      "Epoch 70/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6641 - acc: 0.6061\n",
      "Epoch 71/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6636 - acc: 0.6242\n",
      "Epoch 72/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6597 - acc: 0.6364\n",
      "Epoch 73/100\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.6598 - acc: 0.6121\n",
      "Epoch 74/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6585 - acc: 0.6121\n",
      "Epoch 75/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6573 - acc: 0.6242\n",
      "Epoch 76/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6564 - acc: 0.6364\n",
      "Epoch 77/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6569 - acc: 0.6364\n",
      "Epoch 78/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6557 - acc: 0.6364\n",
      "Epoch 79/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6557 - acc: 0.6424\n",
      "Epoch 80/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6546 - acc: 0.6303\n",
      "Epoch 81/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6576 - acc: 0.6061\n",
      "Epoch 82/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6522 - acc: 0.6303\n",
      "Epoch 83/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6515 - acc: 0.6424\n",
      "Epoch 84/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6507 - acc: 0.6303\n",
      "Epoch 85/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6497 - acc: 0.6242\n",
      "Epoch 86/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6487 - acc: 0.6303\n",
      "Epoch 87/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6485 - acc: 0.6303\n",
      "Epoch 88/100\n",
      "165/165 [==============================] - 0s 73us/step - loss: 0.6482 - acc: 0.6424\n",
      "Epoch 89/100\n",
      "165/165 [==============================] - 0s 73us/step - loss: 0.6478 - acc: 0.6121\n",
      "Epoch 90/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6480 - acc: 0.6121\n",
      "Epoch 91/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6476 - acc: 0.6303\n",
      "Epoch 92/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6445 - acc: 0.6182\n",
      "Epoch 93/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6448 - acc: 0.6303\n",
      "Epoch 94/100\n",
      "165/165 [==============================] - 0s 73us/step - loss: 0.6414 - acc: 0.6242\n",
      "Epoch 95/100\n",
      "165/165 [==============================] - 0s 73us/step - loss: 0.6412 - acc: 0.6303\n",
      "Epoch 96/100\n",
      "165/165 [==============================] - 0s 85us/step - loss: 0.6403 - acc: 0.6364\n",
      "Epoch 97/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6419 - acc: 0.6242\n",
      "Epoch 98/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6427 - acc: 0.6485\n",
      "Epoch 99/100\n",
      "165/165 [==============================] - 0s 73us/step - loss: 0.6371 - acc: 0.6485\n",
      "Epoch 100/100\n",
      "165/165 [==============================] - 0s 79us/step - loss: 0.6362 - acc: 0.6545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19291f32da0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43032944]] 0\n",
      "[[0.41325268]] 0\n",
      "[[0.50476813]] 1\n",
      "[[0.51783216]] 0\n",
      "[[0.44451445]] 0\n",
      "[[0.47938728]] 0\n",
      "[[0.5251898]] 1\n",
      "[[0.43599787]] 0\n",
      "[[0.41483465]] 0\n",
      "[[0.39645946]] 0\n",
      "[[0.48692605]] 1\n",
      "[[0.51940435]] 0\n",
      "[[0.5317897]] 1\n",
      "[[0.42635778]] 0\n",
      "[[0.4649045]] 1\n",
      "[[0.5066088]] 1\n",
      "[[0.45701942]] 1\n",
      "[[0.4155106]] 0\n",
      "[[0.40865436]] 0\n",
      "[[0.44721675]] 0\n",
      "[[0.45939857]] 0\n",
      "[[0.54690605]] 1\n",
      "[[0.50995386]] 0\n",
      "[[0.5383193]] 0\n",
      "[[0.6188475]] 1\n",
      "[[0.7228137]] 0\n",
      "[[0.47560754]] 1\n",
      "acc: 0.6296296296296297\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for i in range(len(X_test)):\n",
    "    X = np.array(X_test[i])\n",
    "    X = [[X]]\n",
    "    y = y_test[i]\n",
    "    output = model.predict(X)\n",
    "    print(output, y)\n",
    "    sign = 1 if output[0] > 0.6 else 0\n",
    "    stats.append(1 if  sign == y else 0)\n",
    "\n",
    "print(\"acc: \" + str(sum(stats) / len(stats)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
